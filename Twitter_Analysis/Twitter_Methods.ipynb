{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods for Twitter Processing and Sentiment Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import sys, os\n",
    "import string\n",
    "import re\n",
    "import datetime\n",
    "import warnings\n",
    "# Natural Language Processing\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem.snowball import EnglishStemmer # load the stemmer module from NLTK\n",
    "dataloc = '/Volumes/Seagate Backup Plus Drive/PoliTweet/TwitterData/'\n",
    "import emoji\n",
    "# Sci-Kit Learn\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "twtokenizer = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmer = EnglishStemmer() # Get an instance of SnowballStemmer for English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "punctuation = list(set(string.punctuation)) + ['…','’','...','—',':/','”','..', '“']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours',\n",
    "'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers',\n",
    "'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves',\n",
    "'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are',\n",
    "'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does',\n",
    "'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until',\n",
    "'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into',\n",
    "'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down',\n",
    "'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here',\n",
    "'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\n",
    "'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so',\n",
    "'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should'] # Removed 'not','now', 'no','nor'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwords_short = ['the','an','a', ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweet Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Working on Reading In Data from New File Structure\n",
    "# Update to Read in Files only from Selected Date Range\n",
    "def getFileList(candidate,orgDataLoc):\n",
    "    'Get Full Path to all Files in the Organized Twitter Data Set for a Candidate'\n",
    "    filelist = []\n",
    "    dataloc = orgDataLoc+candidate+'/by_script_date/'\n",
    "    walk = os.walk(dataloc)\n",
    "    for dirpath, dirs, files in walk:\n",
    "        for f in files:\n",
    "            #print(f)\n",
    "            if f != '.DS_Store':\n",
    "                filelist.append((candidate,(dirpath+'/'+f)))\n",
    "    if len(filelist) == 0:\n",
    "        warnings.warn(\"Warning: No Files Found at the Input Location\")\n",
    "    return filelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readTwitterCSV(file):\n",
    "    'Gets Tweets as Pandas DataFrame, Very Slow'\n",
    "    data = pd.DataFrame(list(csv.reader(open(file),skipinitialspace=True)))\n",
    "    newCol = []\n",
    "    counter = 0\n",
    "    for word in data.iloc[0]:\n",
    "        if word is None:\n",
    "            word = 'None'+'_'+ str(counter)\n",
    "            newCol.append(word)\n",
    "            counter += 1\n",
    "        else:\n",
    "            newCol.append(word)\n",
    "    \n",
    "    data.columns = newCol\n",
    "    \n",
    "    data = data.drop(0)\n",
    "    data.dropna(axis = 0)\n",
    "    return data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readTwitterCSVaslines(file):\n",
    "    'Get Tweet Text and Date from Tweets in File, Returns Dict where Tweet ID is the Key'\n",
    "    data_dict = {}\n",
    "    rowcount = 0\n",
    "    with open(file, errors='ignore') as f:\n",
    "        content = f.readlines()\n",
    "    \n",
    "    # Check to make sure no Null Bytes in Tweet Data Row, if True, drop Row\n",
    "    content_noNull = [row for row in content if row.find('\\x00') == -1 ]\n",
    "    diff = len(content) - len(content_noNull)\n",
    "    if diff != 0:\n",
    "        print(\"Dropped \"+str(diff)+\" Row from \"+file+\" due to Null Bytes\")\n",
    "\n",
    "    for row in csv.reader(content_noNull, skipinitialspace=True):\n",
    "        if rowcount != 0:\n",
    "            key = row[4] # statusId\n",
    "            # print(key)\n",
    "            data_dict[key] = (row[11],row[7]) # (statusText,statusCreatedAt)\n",
    "            # print(data_dict[key])\n",
    "        rowcount+=1\n",
    "        \n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_tweets(tweet):\n",
    "    # Need to First Clean Out URLs before Tokenization\n",
    "    tweet = re.sub('htt[^ ]*' ,'URL', tweet)\n",
    "    \n",
    "    #tokenizer = nltk.tokenize.treebank.TreebankWordTokenizer()\n",
    "    cleanWords = twtokenizer.tokenize(tweet)\n",
    "    \n",
    "    # Convert to Lowercase\n",
    "    cleanWords = [t.lower() for t in cleanWords]\n",
    "    \n",
    "    # Convert Emoji's to Word Label\n",
    "    cleanWords = [emoji.demojize(word) for word in cleanWords]\n",
    "    \n",
    "\n",
    "    # Normalize (remove punctuation)\n",
    "    #Remove punctuation\n",
    "    cleanWords = [word for word in cleanWords if word not in punctuation]\n",
    "    \n",
    "    # punc = string.punctuation\n",
    "    # cleanWords = [t for t in cleanWords if t not in punc]\n",
    "    # cleanWords = [re.sub('[^0-9a-z]', \"\", x) for x in cleanWords]\n",
    "    \n",
    "    # Remove Empty Vectors\n",
    "    cleanWords = [x for x in cleanWords if x != '']\n",
    "     \n",
    "    # Remove StopWords\n",
    "    # cleanWords = [word for word in cleanWords if word not in stopwords_short]\n",
    "    cleanWords = [word for word in cleanWords if word not in stopwords]\n",
    "    \n",
    "    # Identify Digits & Convert to Num\n",
    "    # cleanWords = [re.sub(\"\\d+\", \"NUM\", x) for x in cleanWords]\n",
    "    \n",
    "    # Remove all Web/URL References (Replace with String Replacement Above)\n",
    "    # Could be better to replace with 'URL'\n",
    "    # cleanWords = [word for word in cleanWords if word[0:3] != 'htt']\n",
    "    # cleanWords = ['URL' if word[0:3] == 'htt' else word for word in cleanWords ]\n",
    "    \n",
    "    # Stem Words\n",
    "    #cleanWords = [stemmer.stem(x) for x in cleanWords] # call stemmer to stem the input\n",
    "    \n",
    "    # Remove Multiple Letters, Replace with only 3 so they are distinguishable, but standardized\n",
    "    # cleanWords = [re.sub(r'(.)\\1{2,}', r'\\1\\1\\1', word) for word in cleanWords ]\n",
    "    \n",
    "    # Change all @ References to USER\n",
    "    # cleanWords = ['USER' if word[0] == '@' else word for word in cleanWords ]\n",
    "    \n",
    "    \n",
    "    return cleanWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_tweets_opt(tweet, lower = True, demoji = True, punc = True, stopwords = [], \\\n",
    "                     num = False, url = True, stem = False, repeatedChar = False, users = False):\n",
    "    # Need to Clean Out URLs before Tokenization\n",
    "    if url:\n",
    "        tweet = re.sub('htt[^ ]*' ,'URL', tweet)\n",
    "    \n",
    "    #tokenizer = nltk.tokenize.treebank.TreebankWordTokenizer()\n",
    "    cleanWords = twtokenizer.tokenize(tweet)\n",
    "    \n",
    "    # lower\n",
    "    # Convert to Lowercase\n",
    "    if lower:\n",
    "        cleanWords = [word.lower() for word in cleanWords]\n",
    "    \n",
    "    # demoji\n",
    "    # Convert Emoji's to Word Label\n",
    "    if demoji:\n",
    "        cleanWords = [emoji.demojize(word) for word in cleanWords]\n",
    "\n",
    "    # punc\n",
    "    # Remove punctuation, only removes puncutation if only char in token\n",
    "    if punc:\n",
    "        cleanWords = [word for word in cleanWords if word not in punctuation]\n",
    "     \n",
    "    # Remove StopWords\n",
    "    # Preferred list passed through function parameters\n",
    "    cleanWords = [word for word in cleanWords if word not in stopwords]\n",
    "    \n",
    "    # num\n",
    "    # Identify Digits & Convert to Num\n",
    "    if num:\n",
    "        cleanWords = [re.sub(\"\\d+\", \"NUM\", x) for x in cleanWords]\n",
    "    \n",
    "    # url; opt = remove, replace\n",
    "    # Remove all Web/URL References\n",
    "    #if url:\n",
    "    #    cleanWords = [word for word in cleanWords if word[0:3] != 'htt']\n",
    "    # cleanWords = ['URL' if word[0:3] == 'htt' else word for word in cleanWords ]\n",
    "    \n",
    "    # stem\n",
    "    # Stem Words\n",
    "    if stem:\n",
    "        cleanWords = [stemmer.stem(x) for x in cleanWords] # call stemmer to stem the input\n",
    "    \n",
    "    # repeatedChar\n",
    "    # Remove Multiple Letters, Replace with only 3 so they are distinguishable, but standardized\n",
    "    if repeatedChar:\n",
    "        cleanWords = [re.sub(r'(.)\\1{2,}', r'\\1\\1\\1', word) for word in cleanWords ]\n",
    "    \n",
    "    # users\n",
    "    # Change all @ References to USER\n",
    "    if users:\n",
    "        cleanWords = ['USER' if word[0] == '@' else word for word in cleanWords ]\n",
    "    \n",
    "    ## Non-Optional Pre-processing\n",
    "    # Trim whitespace\n",
    "    \n",
    "    \n",
    "    # Remove Empty Vectors\n",
    "    cleanWords = [x for x in cleanWords if x != '']\n",
    "    \n",
    "    return cleanWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getHTMLcodes(emojis2decode, codesDict):\n",
    "    'Convert Emoji Labels to HTML codes for rendering on webpage'\n",
    "    uni = [str(emoji.emojize(e).encode('unicode-escape')) for e in emojis2decode]\n",
    "    uni_clean = [u.replace(\"\\\\\",\"\").replace(\"b'\",\"\").replace(\"'\",\"\").replace(\"U000\",\"&#x\") for u in uni]\n",
    "    uni_clean = ['&#x'+u[1:len(u)] if u[0] == 'u' else u for u in uni_clean]\n",
    "\n",
    "    # codesDict = {}\n",
    "    # [codes[c]['html']={} for c,u in list(zip(emoji_counts_DF.columns, uni_clean))]\n",
    "    for c,u in list(zip(emojis2decode, uni_clean)):\n",
    "        if c not in codesDict.keys():\n",
    "            codesDict[c]={}\n",
    "            codesDict[c]['htmlcode']={}\n",
    "            codesDict[c]['htmlcode']=u\n",
    "    return codesDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_ads(ad):\n",
    "    # print(type(ad))\n",
    "    #tokenizer = nltk.tokenize.treebank.TreebankWordTokenizer()\n",
    "    cleanWords = twtokenizer.tokenize(ad)\n",
    "    \n",
    "    # Convert to Lowercase\n",
    "    cleanWords = [t.lower() for t in cleanWords]\n",
    "    \n",
    "    # Convert Emoji's to Word Label\n",
    "    # cleanWords = [emoji.demojize(word) for word in cleanWords]\n",
    "    \n",
    "\n",
    "    # Normalize (remove punctuation)\n",
    "    #Remove punctuation\n",
    "    cleanWords = [word for word in cleanWords if word not in punctuation]\n",
    "    \n",
    "    # punc = string.punctuation\n",
    "    # cleanWords = [t for t in cleanWords if t not in punc]\n",
    "    # cleanWords = [re.sub('[^0-9a-z]', \"\", x) for x in cleanWords]\n",
    "    \n",
    "    # Remove Empty Vectors\n",
    "    cleanWords = [x for x in cleanWords if x != '']\n",
    "     \n",
    "    # Remove StopWords\n",
    "    # cleanWords = [word for word in cleanWords if word not in stopwords_short]\n",
    "    cleanWords = [word for word in cleanWords if word not in stopwords]\n",
    "    \n",
    "    # Identify Digits & Convert to Num\n",
    "    #cleanWords = [re.sub(\"\\d+\", \"NUM\", x) for x in cleanWords]\n",
    "    \n",
    "    # Remove all Web/URL References\n",
    "    # Could be better to replace with 'URL'\n",
    "    # cleanWords = [word for word in cleanWords if word[0:3] != 'htt']\n",
    "    \n",
    "    # Stem Words\n",
    "    #cleanWords = [stemmer.stem(x) for x in cleanWords] # call stemmer to stem the input\n",
    "    \n",
    "    # Remove Multiple Letters, Replace with only 3\n",
    "    \n",
    "    \n",
    "    return cleanWords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daily Count Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_emoji_counts(master, emoji_counts, candidate):\n",
    "    if candidate not in emoji_counts.keys():\n",
    "        emoji_counts[candidate] = {}\n",
    "    for key in master.keys():\n",
    "        tweet = master[key][0]\n",
    "        date = master[key][1]\n",
    "        date = datetime.datetime.strptime(date,'%a %b %d %H:%M:%S %Z %Y')\n",
    "        date_ft = date.strftime('%m-%d-%Y')\n",
    "        \n",
    "        # Replace all URLs in Tweet (to avoid confusion with emoticon)\n",
    "        tweet = re.sub('htt[^ ]*' ,'URL', tweet)\n",
    "        \n",
    "        tokens = twtokenizer.tokenize(tweet)\n",
    "        tokens = [emoji.demojize(token) for token in tokens]\n",
    "        # tokens = [word for word in tokens if word not in string.punctuation]\n",
    "\n",
    "        for token in tokens:\n",
    "            if re.match(':+[a-z_]*:*',token):\n",
    "                if date_ft not in emoji_counts[candidate].keys():\n",
    "                    emoji_counts[candidate][date_ft] = {}\n",
    "                if token in emoji_counts[candidate][date_ft]:\n",
    "                    emoji_counts[candidate][date_ft][token] +=1\n",
    "                else:\n",
    "                    emoji_counts[candidate][date_ft][token] = 1\n",
    "    return emoji_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_daily_tweet_counts(master, counts, candidate):\n",
    "    'Get number of Tweets for each day'\n",
    "    if candidate not in counts.keys():\n",
    "        counts[candidate] = {}\n",
    "    for key in master.keys():\n",
    "        #tweet = master[key][0]\n",
    "        #print(tweet)\n",
    "        date = master[key][1]\n",
    "        date = datetime.datetime.strptime(date,'%a %b %d %H:%M:%S %Z %Y')\n",
    "        date_ft = date.strftime('%Y-%m-%d')\n",
    "\n",
    "        if date_ft in counts[candidate]:\n",
    "            counts[candidate][date_ft] +=1\n",
    "        else:\n",
    "            counts[candidate][date_ft] = 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_daily_tweet_counts_PAR(master, candidate):\n",
    "    'Get number of Tweets for each day for use with parallel package, creates new dictionary'\n",
    "    counts = {}\n",
    "    if candidate not in counts.keys():\n",
    "        counts[candidate] = {}\n",
    "    for key in master.keys():\n",
    "        # tweet = master[key][0]\n",
    "        date = master[key][1]\n",
    "        date = datetime.datetime.strptime(date,'%a %b %d %H:%M:%S %Z %Y')\n",
    "        date_ft = date.strftime('%Y-%m-%d')\n",
    "\n",
    "        if date_ft in counts[candidate]:\n",
    "            counts[candidate][date_ft] +=1\n",
    "        else:\n",
    "            counts[candidate][date_ft] = 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_hashtag_counts(master, emoji_counts, candidate):\n",
    "    if candidate not in emoji_counts.keys():\n",
    "        emoji_counts[candidate] = {}\n",
    "    for key in master.keys():\n",
    "        tweet = master[key][0]\n",
    "        date = master[key][1]\n",
    "        date = datetime.datetime.strptime(date,'%a %b %d %H:%M:%S %Z %Y')\n",
    "        date_ft = date.strftime('%m-%d-%Y')\n",
    "        \n",
    "        # Replace all URLs in Tweet (to avoid confusion with emoticon)\n",
    "        tweet = re.sub('htt[^ ]*' ,'URL', tweet)\n",
    "        \n",
    "        tokens = twtokenizer.tokenize(tweet)\n",
    "        \n",
    "        # Only Tokens with '#' sign\n",
    "        hashtags = [token.lower() for token in tokens if \"#\" in token]\n",
    "        # tokens = [word for word in tokens if word not in string.punctuation]\n",
    "\n",
    "        \n",
    "        for token in hashtags:\n",
    "            # print(token)\n",
    "            if date_ft not in emoji_counts[candidate].keys():\n",
    "                emoji_counts[candidate][date_ft] = {}\n",
    "            if token in emoji_counts[candidate][date_ft]:\n",
    "                emoji_counts[candidate][date_ft][token] +=1\n",
    "            else:\n",
    "                emoji_counts[candidate][date_ft][token] = 1\n",
    "    return emoji_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NB Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getTermFreq(tokenList):\n",
    "    TF = {}\n",
    "    #print(word)\n",
    "    for word in tokenList:\n",
    "        #print(word)\n",
    "        if word in TF:\n",
    "            TF[word] += 1\n",
    "        else:\n",
    "            TF[word] = 1\n",
    "    return TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_vocabulary(textlist):\n",
    "    vocab = {}\n",
    "    for row in textlist:\n",
    "        for word in row:\n",
    "            if word in vocab:\n",
    "                vocab[word] += 1\n",
    "            else:\n",
    "                vocab[word] = 1\n",
    "    return vocab   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getDocFreq(textlist):\n",
    "    DF = {}\n",
    "    for row in textlist:\n",
    "        for word in set(row):\n",
    "            # print(word)\n",
    "            if word in DF:\n",
    "                DF[word] += 1\n",
    "            else:\n",
    "                DF[word] = 1\n",
    "    return DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_countVectors(tokens):\n",
    "    doc_TF = {}\n",
    "    for token in tokens:\n",
    "        if token in doc_TF:\n",
    "            doc_TF[token] += 1\n",
    "        else:\n",
    "            doc_TF[token] = 1\n",
    "    return doc_TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Unigram Language Model\n",
    "def genUniLM(TF):\n",
    "    u_theta = pd.DataFrame.from_dict(TF, orient = \"index\")\n",
    "    u_theta.columns = ['TF']\n",
    "    # u_theta.sort('TF', ascending = False)[0:10]\n",
    "    # Total Number of Words in Training Corpus\n",
    "    nWords = u_theta['TF'].sum()\n",
    "    nWords\n",
    "    # Number of Unique Words in Training Corpus\n",
    "    vSize = len(u_theta['TF'])\n",
    "    vSize\n",
    "    # Calculate Probabilty of Each Word by TTF/N\n",
    "    u_theta['p'] = u_theta/nWords\n",
    "    u_theta = u_theta.sort('TF', ascending = False)\n",
    "    # Check that Probability Sums to 1\n",
    "    print(\"Total Probability: \",u_theta['p'].sum())\n",
    "    return u_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_pSmoothAdditive(tokenList, u_theta, d):\n",
    "    \n",
    "    vSize_train = len(u_theta)\n",
    "    nWords_train = sum(u_theta['TF'])\n",
    "    \n",
    "    unseenWords = list(set(tokenList) - set(u_theta.index))\n",
    "    #print(len(unseenWords))\n",
    "    if len(unseenWords) == 0:\n",
    "        return u_theta['p']\n",
    "    else:\n",
    "        # Build Series with all unique words in training set + unseen words from test document\n",
    "        pSmooth = u_theta['TF'].append(pd.Series(([0]*len(unseenWords)), index = unseenWords))\n",
    "        nWords_train += len(unseenWords)\n",
    "        vSize_train += len(unseenWords)\n",
    "        f = lambda x: ((x + d) / (nWords_train + d*vSize_train))\n",
    "        pSmooth = pSmooth.map(f)\n",
    "        return pSmooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calcfX_NBLin(tokenlist, prob1, prob0, pSmooth_1, pSmooth_0):\n",
    "    X = 0\n",
    "    for word in tokenlist:\n",
    "        try:\n",
    "            pX_y1 = pSmooth_1.ix[word]\n",
    "        except KeyError:\n",
    "            pX_y1 = 1\n",
    "        try:\n",
    "            pX_y0 = pSmooth_0.ix[word]\n",
    "        except KeyError:\n",
    "            pX_y0 = 1\n",
    "            \n",
    "        x = math.log(pX_y1)-math.log(pX_y0)\n",
    "        X = X + x\n",
    "    fX = math.log(prob1/prob0) + X\n",
    "\n",
    "    return fX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sampleKeys(dict):\n",
    "    return list(dict)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [poli]",
   "language": "python",
   "name": "Python [poli]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
